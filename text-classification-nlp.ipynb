{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Text Classification - Natural Language Processing\n\nText classification also known as *text tagging* or *text categorization* is the process of categorizing text into organized groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of pre-defined tags or categories based on its content.\n\nThis kernel aims to give  a brief overview of performing text classification using Naive Bayes, Logistic Regression, Support Vector Machines and Decision Tree Classifier. We will be using a dataset called \"**Economic news article tone and relevance**\" which consists of approximately 8000 news articles, which were tagged as ***relevant*** or ***not relevant*** to the US Economy. Our goal in this kernel is to explore the process of training and testing text classifiers for this dataset.","metadata":{}},{"cell_type":"markdown","source":"![Text Classification](https://www.dataquest.io/wp-content/uploads/2019/04/text-classification-python-spacy.png)","metadata":{}},{"cell_type":"markdown","source":"### Import Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport matplotlib as mpl \nimport matplotlib.cm as cm \nimport matplotlib.pyplot as plt \nimport plotly.graph_objects as go\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction import stop_words\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\nimport string\nimport re\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\n\nfrom time import time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis of Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/us-economic-news-articles/US-Economic-News.csv', encoding = 'ISO-8859-1')\ndisplay(data.shape) ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"relevance\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an imbalance in the data with not relevant being 82% in the dataset. That is, most of the articles are not relevant to US Economy, which makes sense in a real-world scenario, as news articles discuss various topics. We should keep this class imbalance mind when interpreting the classifier performance later. Let us first convert the class labels into binary outcome variables for convenience. 1 for Yes (relevant), and 0 for No (not relevant), and ignore \"Not sure\".","metadata":{}},{"cell_type":"code","source":"data = data[data.relevance != \"not sure\"]\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"relevance\"].value_counts()/data.shape[0] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure([go.Bar(x=data['relevance'].value_counts().index, y=data['relevance'].value_counts().tolist())])\nfig.update_layout(\n    title=\"Values in each Sentiment\",\n    xaxis_title=\"Sentiment\",\n    yaxis_title=\"Values\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Pre-processing","metadata":{}},{"cell_type":"code","source":"data['relevance'] = data.relevance.map({'yes':1, 'no':0}) # relevant is 1, not-relevant is 0 \ndata = data[[\"text\",\"relevance\"]] # taking text input and output variable as relevance\ndata = data[:1000]\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Cleaning","metadata":{}},{"cell_type":"markdown","source":"Typical steps involve tokenization, lower casing, removing, stop words, punctuation markers etc, and vectorization. Other processes such as stemming/lemmatization can also be performed. Here, we are performing the following steps: removing br tags, punctuation, numbers, and stopwords. While we are using sklearn's list of stopwords, there are several other stop word lists (e.g., from NLTK) or sometimes, custom stopword lists are needed depending on the task.","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\nstopwords = stop_words.ENGLISH_STOP_WORDS\nlemmatizer = WordNetLemmatizer()\n\ndef clean(doc):\n    text_no_namedentities = []\n    document = nlp(doc)\n    ents = [e.text for e in document.ents]\n    for item in document:\n        if item.text in ents:\n            pass\n        else:\n            text_no_namedentities.append(item.text)\n    doc = (\" \".join(text_no_namedentities))\n\n    doc = doc.lower().strip()\n    doc = doc.replace(\"</br>\", \" \") \n    doc = doc.replace(\"-\", \" \") \n    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n    doc = \" \".join([token for token in doc.split() if token not in stopwords])    \n    doc = \"\".join([lemmatizer.lemmatize(word) for word in doc])\n    return doc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean(data['text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].apply(clean)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready for the modeling. We are going to use algorithms from sklearn package. We will go through the following steps:\n\n1. Split the data into training and test sets (80% train, 20% test)\n2. Extract features from the training data using TfidfVectorizer.\n3. Transform the test data into the same feature vector as the training data.\n4. Train the classifier\n5. Evaluate the classifier","metadata":{}},{"cell_type":"markdown","source":"## TF-IDF Vectorizer\n\n![TF-IDF](https://miro.medium.com/max/3136/1*ruCawEw0--m2SeHmAQooJQ.jpeg)","metadata":{}},{"cell_type":"code","source":"docs = list(data['text'])\ntfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features = 20000) \ntfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\ndocs = tfidf_vectorizer_vectors.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = docs \ny = data['relevance']\nprint(X.shape, y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure([go.Bar(x=y.value_counts().index, y=y.value_counts().tolist())])\nfig.update_layout(\n    title=\"Values in each Sentiment\",\n    xaxis_title=\"Sentiment\",\n    yaxis_title=\"Values\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Test Split","metadata":{}},{"cell_type":"code","source":"SEED=123\nX_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes Classifier","metadata":{}},{"cell_type":"markdown","source":"### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB() \n%time gnb.fit(X_train, y_train)\n\ny_pred_train = gnb.predict(X_train)\ny_pred_test = gnb.predict(X_test)\nprint(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\nprint(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_test)\n# print('Confusion matrix\\n', cm)\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                        index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = gnb.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gaussian Naive Bayes performs poorly in this case because of the prior and posterior probability condition","metadata":{}},{"cell_type":"markdown","source":"### Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"mnb = MultinomialNB() \n%time mnb.fit(X_train, y_train)\n\ny_pred_train = mnb.predict(X_train)\ny_pred_test = mnb.predict(X_test)\nprint(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\nprint(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_test)\n# print('Confusion matrix\\n', cm)\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                        index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = mnb.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Multinomial Naive Bayes performs slightly better than Gaussian Naive Bayes, but the low AUC score is because the size of feature vector is really big and Bayes Algorothm works better for small number of features. Let's check out results of Logistic Regression, Support Vector Machines and Decision Tree Classifier.","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(random_state=SEED)\n%time lr.fit(X_train, y_train)\n\ny_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)\nprint(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\nprint(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_test)\n#print('Confusion matrix\\n', cm)\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                        index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = lr.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Machines","metadata":{}},{"cell_type":"code","source":"svc =  LinearSVC(class_weight='balanced') \n%time svc.fit(X_train, y_train)\n\ny_pred_train = svc.predict(X_train)\ny_pred_test = svc.predict(X_test)\nprint(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\nprint(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_test)\n# print('Confusion matrix\\n', cm)\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                        index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = svc._predict_proba_lr(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=SEED)\n%time dt.fit(X_train, y_train)\n\ny_pred_train = dt.predict(X_train)\ny_pred_test = dt.predict(X_test)\nprint(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\nprint(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_test)\n# print('Confusion matrix\\n', cm)\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                        index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nclassifiers = [('Decision Tree', dt),\n               ('Logistic Regression', lr),\n                ('Naive Bayes', gnb)\n              ]\nvc = VotingClassifier(estimators=classifiers)\n# Fit 'vc' to the traing set and predict test set labels\nvc.fit(X_train, y_train)\ny_pred_train=vc.predict(X_train)\ny_pred_test = vc.predict(X_test)\nprint(\"Training Accuracy score:\",accuracy_score(y_train, y_pred_train))\nprint(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.Series(vc.predict(X), name=\"relevant\")\nresults = pd.concat([predictions],axis=1)\nresults.to_csv(\"us-economic-news-relevance.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nSo, how do we choose whats the best? If we look at overall accuracy alone, we should be choosing the very first classifier in this notebook. However, that is also doing poorly with identifying \"relevant\" articles. If we choose purely based on how good it is doing with \"relevant\" category, we should choose the second one we built. If we choose purely based on how good it is doing with \"irrelevant\" category, surely, nothing beats not building any classifier and just calling everything irrelevant! So, what to choose as the best among these depends on what we are looking for in our usecase!\n\nLet me know how you found this notebook and share your feedback in the comments section. Happy Kaggling :)","metadata":{}}]}